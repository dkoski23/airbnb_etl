# etl_challenge
https://www.kaggle.com/mysarahmadbhat/airbnb-listings-reviews (this has the csvs I used).

This project is a combination of extracting data from the internet, transforming it using pandas, and loading it into an SQL database via using SQL Alchemy to connect to pgAdmin.

In this project I extracted the Airbnb Review and Listings data from the above link. The next step was to read the files into pandas using a Jupyter notebook and examine the data in further detail. At this point I decided to separate the Listings dataframe into several smaller dataframes based upon listing id, host id, and host location (basically, the details the host provides as to where the listing takes place), and made an ERB to refer to during the process of writing code. I used the listing ids in the reviews dataframe as a method of calculating total reviews per listing id, host id, and host location. Additionally, it seemed fitting to include review scores for each value in these respective categories. If anyone in the company were interested in examining the review scores based upon any of this criteria, it would now be easy. Additionally, it is more secure for a company to have important data such as this spread throughout several tables that are not highly dependent on one another in case any data gets accidentally dropped or hacked (I was one the fence about including the listings_info table, but figured it would be helpful for visualizing the connections). Once the tables were formatted in SQL and the data cleaned using pandas, SQL Alchemy was used to load the pandas dataframes into the existing database schema within pgAdmin.

The format of the tables was created using pgAdmin, and the code for this formatting is in the etl_schema.sql file. The ERB code is at the bottom of the reading_etl_csvs.ipynb notebook, the rest of which I just used to get a better look at the data. All of the pandas cleaning/restructuring was done in the etl_project.ipynb notebook. The csvs are not on github because they are too large. Even though the tables I created are smaller, most of them have hundreds of thousands or millions of rows, so depending on one's system it may take a while to run. The computer this code was run on has a lot of working memory, so loading the data into the database did not take long.
